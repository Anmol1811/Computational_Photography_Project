{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvBDt2VB65p6"
   },
   "source": [
    "Notebook that\n",
    "- extract features from a training split for image transformation.\n",
    "- can transform images from one emotion to the other by:\n",
    "  - finding the emotions that are relevant to the emotion. Therefore, we use the ground truth probabilities and correlate them to the feature values. The p-values are then used to determine if the feature is relevant.\n",
    "  - then the mean features for each emotion are calculated and used as the target values for the transformation\n",
    "  - each test image is transferred to each emotion that is not the most probable emotion (in total 7 emotions so each image is transferred to 6 target emotions)\n",
    "  - the transformation itself is done by first picking the features that are relevant to target and original emotion (using p-values) then the image is altered such that the features that are relevant match the mean features of the target emotion.\n",
    "\n",
    "After the transformation is done we classify both the input and the altered image with Anmol's classification model. We get a label for the most likely emotion\n",
    "Performance is evaluated by: \n",
    "  - precision based on the label\n",
    "  - if the amount of images classified as the target emotion has increased\n",
    "  - how many images were classified as the target emotion before vs after the transformation for each target emotion\n",
    "\n",
    "We evaluate different training/test split:\n",
    "- use the images that were used for training of the model for feature extraction and the validation set for testing\n",
    "- the same but using only images that have major prob < 0.5 for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmgFSLccuu7U"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YwFxxmg0uroD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Theresa\\\\Documents\\\\CS_Master\\\\EPFL\\\\CP\\\\Computational_Photography_Project'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from os import listdir\n",
    "from os.path import join as p_join\n",
    "from os.path import abspath\n",
    "from pathlib import Path \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.backends.backend_pdf\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "import glob\n",
    "\n",
    "\n",
    "#%cd /content/drive/MyDrive/CP/Computational Photography\n",
    "from preprocess_emotion6 import get_split_from_folder, dataframe_from_model, threshold_images\n",
    "from feature_extraction import calculateGLCMFeatures, calculateHSVFeatures, calculateLaplacian, rgb_values, rms_contrast, calculateFeatures\n",
    "from feature_transform import change_laplacian, change_bgr, change_hsv\n",
    "PROJDIR = abspath('')\n",
    "PROJDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hyS_DxBzuxLO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Theresa\\Documents\\CS_Master\\EPFL\\CP\\Computational_Photography_Project\n"
     ]
    }
   ],
   "source": [
    "# put the path to the folder here \n",
    "# if this is a shared folder: right click on the folder in \"Shared with me\" directory, and then click \"Add shortcut to Drive\". \n",
    "# then you can access the data from your drive!\n",
    "#PROJDIR = '/content/drive/MyDrive/CP/Computational Photography'\n",
    "print(PROJDIR)\n",
    "DATA = p_join(PROJDIR, 'data') # here could be the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cZCT-CBgu8Tk"
   },
   "outputs": [],
   "source": [
    "# emotion6 dataset\n",
    "emotions_e6 = ['anger', 'disgust', \"fear\", \"joy\", \"sadness\", \"surprise\"]\n",
    "emotions=['anger','disgust','fear','joy','sadness','surprise','neutral']\n",
    "\n",
    "# folder that contains subfolder for each emotion\n",
    "EM = p_join(DATA, 'Emotion6')\n",
    "EM_all = p_join(EM, 'images')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ad84lBRiR4Rd"
   },
   "source": [
    "install the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xK1mq5wmAFy4"
   },
   "outputs": [],
   "source": [
    "%ls\n",
    "%cd /content/drive/MyDrive/CP/Computational Photography\n",
    "%cd artemis\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oemYIcyd9PiJ"
   },
   "outputs": [],
   "source": [
    "from google.colab.patches import cv2_imshow\n",
    "from artemis.neural_models.resnet_encoder import ResnetEncoder\n",
    "from artemis.neural_models.image_emotion_clf import ImageEmotionClassifier\n",
    "from artemis.neural_models.mlp import MLP\n",
    "from PIL import Image\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import multiprocessing as mp\n",
    "import torchvision.transforms as transforms\n",
    "from artemis.in_out.neural_net_oriented import torch_load_model, torch_save_model, save_state_dicts\n",
    "import os.path as osp\n",
    "\n",
    "save_dir = './model_temp'  # for trained model\n",
    "checkpoint_file = osp.join(save_dir, 'our_model_em6.pt')\n",
    "\n",
    "model = torch_load_model(checkpoint_file)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCdHsP-Y6JKN"
   },
   "source": [
    "read ground truth for images\n",
    "\n",
    "1. dataframe that tells which images have been used for the training and validation of the classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "A76yPRVccsXd"
   },
   "outputs": [],
   "source": [
    "emotion_distr_anmol = pd.read_csv(p_join(PROJDIR, \"emotion6_filtered_split.csv\"), sep=',')\n",
    "emotion_distr_anmol.columns = [\"Unnamed\", \"folder\", \"image\", \"emotion_distribution\", \"split\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSNTrNdBc7gv"
   },
   "source": [
    "2. the ground truth data frame given with the emotion6 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FU1aA-D0u9OV"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder</th>\n",
       "      <th>image</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>disgust</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>surprise</td>\n",
       "      <td>1</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>joy</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sadness</td>\n",
       "      <td>1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>joy</td>\n",
       "      <td>329</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>sadness</td>\n",
       "      <td>329</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.072222</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>disgust</td>\n",
       "      <td>330</td>\n",
       "      <td>4.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>joy</td>\n",
       "      <td>330</td>\n",
       "      <td>7.3</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>sadness</td>\n",
       "      <td>330</td>\n",
       "      <td>3.9</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.122222</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1980 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        folder  image  valence  arousal  anger   disgust  fear   joy  sadness  \\\n",
       "0      disgust      1      2.5      3.8   0.13  0.700000  0.00  0.07     0.10   \n",
       "1     surprise      1      6.3      5.7   0.00  0.066667  0.17  0.23     0.13   \n",
       "2         fear      1      2.8      4.8   0.10  0.100000  0.53  0.00     0.13   \n",
       "3          joy      1      8.0      4.5   0.00  0.000000  0.00  0.67     0.00   \n",
       "4      sadness      1      2.7      5.2   0.00  0.000000  0.10  0.00     0.90   \n",
       "...        ...    ...      ...      ...    ...       ...   ...   ...      ...   \n",
       "1975       joy    329      7.7      5.2   0.00  0.000000  0.00  0.62     0.00   \n",
       "1976   sadness    329      4.2      4.6   0.08  0.072222  0.32  0.07     0.31   \n",
       "1977   disgust    330      4.1      5.0   0.02  0.355556  0.17  0.00     0.10   \n",
       "1978       joy    330      7.3      5.2   0.00  0.000000  0.00  0.72     0.00   \n",
       "1979   sadness    330      3.9      6.2   0.09  0.122222  0.02  0.10     0.30   \n",
       "\n",
       "      surprise  neutral  \n",
       "0         0.00     0.00  \n",
       "1         0.23     0.17  \n",
       "2         0.00     0.13  \n",
       "3         0.27     0.07  \n",
       "4         0.00     0.00  \n",
       "...        ...      ...  \n",
       "1975      0.22     0.16  \n",
       "1976      0.02     0.13  \n",
       "1977      0.02     0.33  \n",
       "1978      0.12     0.16  \n",
       "1979      0.23     0.13  \n",
       "\n",
       "[1980 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_distr = pd.read_csv(p_join(PROJDIR, \"em6_groundtruth.csv\"), sep=';')\n",
    "emotion_distr.columns = ['folder', 'image', 'valence', 'arousal', 'anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'neutral']\n",
    "#change image number to integer\n",
    "data_types_dict = {'image': np.int64}\n",
    "# ground truth probability distributions + valence & arousal\n",
    "emotion_distr = emotion_distr.astype(data_types_dict)\n",
    "emotion_distr[(emotion_distr.folder==\"fear\") & (emotion_distr.image==104)]\n",
    "# images\n",
    "emotion_distr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CQvfHTHc-JV"
   },
   "source": [
    "function for extracting the features of images that are contained in the groundtruth dataframe and located in a given path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZLhs7glRvoXA"
   },
   "outputs": [],
   "source": [
    "from gc import get_threshold\n",
    "def extractFeaturesEmotion6(path, emotions: list, groundtruth: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    path: path to training image data\n",
    "    emotions: list of emotions/folders\n",
    "    probabilities: data frame with folder, image id and ground truth distribution for image\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    # for each folder \n",
    "    folders = listdir(path)[:-1]\n",
    "    # for each folder:\n",
    "    for i, f in enumerate(folders):\n",
    "        print(f)\n",
    "        try:\n",
    "            l = listdir(p_join(path, f))\n",
    "        except:\n",
    "            print(f, \"not a folder\")\n",
    "            continue\n",
    "        # for each image:\n",
    "        for file in l:\n",
    "            # image id (name of image in front of .jpg)\n",
    "            image_id = file.split(\".\")[0]\n",
    "            im = cv2.imread(p_join(p_join(path, f), file))\n",
    "            # retrieve ground truth distribution for current image\n",
    "\n",
    "            gt = groundtruth[(groundtruth['folder']==f) & (groundtruth['image']==int(image_id))]\n",
    "            if gt.empty:\n",
    "              continue\n",
    "            # calculate features and append to data frame\n",
    "            probabilities = gt.values[0,4:] # ground truth probabilities\n",
    "            \n",
    "            # compute most likely emotion\n",
    "            label = emotions[np.argmax(probabilities)]\n",
    "            \n",
    "            row = {}\n",
    "            row['folder'] = f\n",
    "            row['image'] = file #image name\n",
    "            row['Emotion'] = label #most likely emotion\n",
    "            row['valence'] = gt.values[0,2]\n",
    "            row['arousal'] = gt.values[0,3]\n",
    "            for i, prob in enumerate(probabilities):\n",
    "                row['prob_'+emotions[i]] = prob # probability for each emotion\n",
    "            # calculate features (im in BGR colors)\n",
    "            row = calculateFeatures(row, im)\n",
    "            # add image results to dataframe\n",
    "            df = df.append(row, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ru7S7OuvtZ6I"
   },
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHafbVwYIetr",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# extract the ground truth for the training split images in anmols model\n",
    "train_em6 = dataframe_from_model(emotion_distr, emotion_distr_anmol, split_class=\"train\")\n",
    "# extract the features\n",
    "df_em6_model = extractFeaturesEmotion6(EM_all, emotions_e6+['neutral'], train_em6)\n",
    "df_em6_model.to_csv(p_join(PROJDIR, \"emotion6_features_trainingsplitanmolsmodel.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "teArJT-Z_wT8"
   },
   "outputs": [],
   "source": [
    "# use all images with a major probability larger than 0.5\n",
    "major_prob_em6 = threshold_images(emotion_distr, threshold = 0.5, greater_than = True)\n",
    "df_em6 = extractFeaturesEmotion6(EM_all, emotions_e6+['neutral'], major_prob_em6)\n",
    "df_em6.to_csv(p_join(PROJDIR, \"emotion6_features_majoremotion.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FplRCPR_6aUm"
   },
   "source": [
    "read already calculated features to avoid having to repeat the previous steps every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdWFIsZNK9O4"
   },
   "outputs": [],
   "source": [
    "df_major = pd.read_csv(p_join(PROJDIR, \"emotion6_features_majoremotion.csv\"), sep=',')\n",
    "df_em6_model = pd.read_csv(p_join(PROJDIR, \"emotion6_features_trainingsplitanmolsmodel.csv\"), sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZ9N6mz-EoIR"
   },
   "source": [
    "### step by step explanation how the feature transformation is done\n",
    "1. extract only the features that can be used for transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SNFcOPSGg8Ti"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau, pearsonr, spearmanr\n",
    "\n",
    "def kendall_pval(x,y):\n",
    "  return kendalltau(x,y)[1]\n",
    "  \n",
    "def pearsonr_pval(x,y):\n",
    "  return pearsonr(x,y)[1]\n",
    "\n",
    "def spearmanr_pval(x,y):\n",
    "  return spearmanr(x,y)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guWNPkSTmseI"
   },
   "source": [
    "assumption: if p value < 0.05 correlation is significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVJWNfRVLsD0"
   },
   "outputs": [],
   "source": [
    "def get_p_val(feature_df, columns=[\"hue\", \"saturation\", \"brightness\", \"Laplacian\", \"blue\", \"green\", \"red\"]):\n",
    "    \"\"\"\n",
    "    get p-values from \"feature_df\" for the features given in \"columns\"\n",
    "    \"\"\"\n",
    "    # if from file\n",
    "    p_val = feature_df.corr(spearmanr_pval)[feature_df.corr().columns[3:10]].T[feature_df.corr().columns[10:]] \n",
    "    #if directly\n",
    "    # feature_df.corr(spearmanr_pval)[feature_df.corr().columns[2:9]].T[feature_df.corr().columns[9:]]\n",
    "    p_val_short = p_val[columns] # hue saturation brightness laplace contrast2 blue green red\n",
    "    return p_val_short\n",
    "\n",
    "p_val_short = get_p_val(df_major)\n",
    "out_pdf = p_join(PROJDIR,'emotion6_pval.pdf')\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(out_pdf)\n",
    "plt.figure(figsize=(9,7))\n",
    "heat_map = sns.heatmap( p_val_short, linewidth = 1 , annot = True, cmap=\"Blues\", vmin=0.0, vmax=0.05)\n",
    "plt.suptitle(\"p-values emotion6\")\n",
    "pdf.savefig()\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijf4W1DofO1c"
   },
   "source": [
    "2. calculate the means of each feature-emotion pair to have a reference to which values we need to change for each emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkG19JZaM406"
   },
   "outputs": [],
   "source": [
    "def get_means_em6(feature_df, emotions_e6, columns=[\"hue\", \"saturation\", \"brightness\", \"Laplacian\", \"blue\", \"green\", \"red\"]):\n",
    "    # choose only features that we can use for transformation (all except most GLCM features):\n",
    "    means = feature_df.groupby(['Emotion']).mean()[columns]\n",
    "    #means.columns = [\"hue\",\t\"saturation\",\t\"brightness\",\t\"Laplacian\",\t\"contrast2\",\t\"blue\",\t\"green\",\t\"red\"]\n",
    "    new_means = means.copy(True)\n",
    "    neutral = means.iloc[4] \n",
    "    sadness = means.iloc[5] \n",
    "    surprise = means.iloc[6]\n",
    "    new_means.iloc[4] = sadness\n",
    "    new_means.iloc[5] = surprise\n",
    "    new_means.iloc[6] = neutral \n",
    "    s = pd.Series(emotions_e6+[\"neutral\"])\n",
    "    new_means = new_means.set_index([s])\n",
    "    return new_means\n",
    "\n",
    "df_em6_means = get_means_em6(df_major, emotions_e6)\n",
    "df_em6_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Im_ffCqxLQGk"
   },
   "source": [
    "3. create mapping dictionary that tells us which features we need to change for each pair of initial and target emotion.\n",
    "\n",
    "function that creates a dictionary that tells us which values we need to change to what degree when we want to transform from one emotion to the other. if the value is 0 it means that we do $\\textbf{not}$ change the feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGFmRkgxAoW0"
   },
   "outputs": [],
   "source": [
    "def create_mapping_dictionary(p_val, means, emotions, use_pval=2):\n",
    "    \"\"\"\n",
    "    returns dataframe that tells which features to change with which value for each pair of original and target emotion\n",
    "    @param p_val p-values that were calculated using the training data of this data set\n",
    "    @param means mean feature values by emotion. Used to determine the amount of change for a mapping of one emotion to the other\n",
    "    @param use_pval in {0,1,2} \n",
    "    0=use all features and ignore p-values\n",
    "    1=use features if p-val of target emotion < 0.05\n",
    "    2=use features only if p-val of initial and target emotion < 0.05\n",
    "    \"\"\"\n",
    "    features = p_val.columns\n",
    "    df = pd.DataFrame()\n",
    "    for original in emotions:\n",
    "        for target in emotions:\n",
    "            if original == target:\n",
    "                continue\n",
    "            row = {}\n",
    "            row['original'] = original\n",
    "            row['target'] = target\n",
    "            # mask that tells whether each feature is relevant for both emotions\n",
    "            relevant_p_val_original = (p_val<0.05).loc[['prob_'+original]].values\n",
    "            relevant_p_val_target = (p_val<0.05).loc[['prob_'+target]].values\n",
    "            overall_p_val = relevant_p_val_target & relevant_p_val_original\n",
    "            # mean values all features\n",
    "            if use_pval == 0:\n",
    "                mean_features_target = (means.loc[[target]].values)[0]\n",
    "            # mean values of the RELEVANT features of TARGET emotion\n",
    "            elif use_pval == 1:\n",
    "                mean_features_target = (means.loc[[target]].values * relevant_p_val_target)[0]\n",
    "            # mean values of the RELEVANT features of TARGET and ORIGINAL emotion\n",
    "            else:\n",
    "                mean_features_target = (means.loc[[target]].values * overall_p_val)[0]\n",
    "            for i, f in enumerate(features):\n",
    "                row[f] = mean_features_target[i]\n",
    "            df = df.append(row, ignore_index=True)\n",
    "    return df\n",
    "mapping = create_mapping_dictionary(p_val_short, df_em6_means, emotions_e6+['neutral'], 2)\n",
    "# example: what features do we need to change to which values to go from joy to sadness?\n",
    "mapping[(mapping['original']=='joy') & (mapping['target']=='sadness')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKS3MSvf5L90"
   },
   "source": [
    "## Feature transformation\n",
    "\n",
    "----\n",
    "first. examples for the transformation functions for RGB, HSV and laplacian.\n",
    "The functions are found in feature_transform.py\n",
    "\n",
    "1. RGB values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8B2e_zTL9S2"
   },
   "outputs": [],
   "source": [
    "# example change bgr\n",
    "image_path = p_join(p_join(EM_all, 'fear'), '108.jpg')\n",
    "\n",
    "# read file and store in temporary file that can be manipulated\n",
    "im = cv2.imread(image_path)\n",
    "folder = image_path.rsplit('/', 2)[0]\n",
    "temp_file = p_join(folder, 'temp.jpg')\n",
    "cv2.imwrite(temp_file, im)\n",
    "\n",
    "#see what it looks like originally \n",
    "im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "print(f\"before: mean blue={im[:,:,0].mean():.2f}, mean green={im[:,:,1].mean():.2f}, mean red={im[:,:,2].mean():.2f}\")\n",
    "\n",
    "# change to the given new means and look at the result\n",
    "target = [20,200,100]\n",
    "print(f\"goal: mean blue={target[0]}, mean green={target[1]}, mean red={target[2]}\")\n",
    "im = change_bgr(temp_file, target) # bgr\n",
    "print(f\"after: mean blue={im[:,:,0].mean():.2f}, mean green={im[:,:,1].mean():.2f}, mean red={im[:,:,2].mean():.2f}\")\n",
    "im = cv2.imread(temp_file, cv2.IMREAD_COLOR)\n",
    "im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXJRyqYKU6Ct"
   },
   "source": [
    "2. HSV values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRMOI1zcMxvc"
   },
   "outputs": [],
   "source": [
    "# example hsv\n",
    "image_path = p_join(p_join(EM_all, 'fear'), '108.jpg')\n",
    "\n",
    "# read file and store in temporary file that can be manipulated\n",
    "im = cv2.imread(image_path)\n",
    "folder = image_path.rsplit('/', 2)[0]\n",
    "temp_file = p_join(folder, 'temp.jpg')\n",
    "cv2.imwrite(temp_file, im)\n",
    "\n",
    "#see what it looks like originally \n",
    "im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "im = cv2.cvtColor(im, cv2.COLOR_RGB2HSV)\n",
    "print(f\"before: mean hue={im[:,:,0].mean():.2f}, mean saturation={im[:,:,1].mean():.2f}, mean value={im[:,:,2].mean():.2f}\", )\n",
    "\n",
    "# change to the given new means and look at the result\n",
    "target = [50,100,120]\n",
    "im = change_hsv(temp_file, target, change_hue=True)\n",
    "print(f\"goal: mean hue={target[0]}, mean saturation={target[1]}, mean brightness={target[2]}\")\n",
    "im = cv2.imread(temp_file)\n",
    "im = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n",
    "print(f\"after: mean hue={im[:,:,0].mean():.2f}, mean saturation={im[:,:,1].mean():.2f}, mean value={im[:,:,2].mean():.2f}\")\n",
    "im = cv2.cvtColor(im, cv2.COLOR_HSV2RGB)\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sKJLMtJU8hO"
   },
   "source": [
    "3. Sharpness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsF4RyiVcu_X"
   },
   "outputs": [],
   "source": [
    "# change laplacian\n",
    "dir = p_join(EM_all, 'fear')\n",
    "count = 0\n",
    "# for the first 10 images\n",
    "for img in listdir(dir):\n",
    "  if count == 3:\n",
    "    break\n",
    "  count += 1\n",
    "  image_path = p_join(dir, img)\n",
    "  im = cv2.imread(image_path)\n",
    "  folder = image_path.rsplit('/', 2)[0]\n",
    "  temp_file = p_join(folder, 'temp.jpg')\n",
    "  cv2.imwrite(temp_file, im)\n",
    "\n",
    "  change_laplacian(temp_file, np.random.randint(0,1000), plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ll4fNb0Xlsyp"
   },
   "source": [
    "function to classify image with classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYR3HVEklC_H"
   },
   "outputs": [],
   "source": [
    "def classify(name, model, emotions):\n",
    "    img=cv2.imread(name)\n",
    "  \n",
    "    img = Image.open(name)\n",
    "    if img.mode is not 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img_transform=image_transformation(256)['train']\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    img_inp = torch.unsqueeze(img_transform(img).to(device),0)\n",
    "\n",
    "    out=model(img_inp)\n",
    "    max_pred = np.argmax(out.detach().cpu().numpy(), 1)    \n",
    "    probabilities = out.cpu().detach().numpy()[0]\n",
    "    return emotions[int(max_pred)], probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHQZPZEBerKE"
   },
   "outputs": [],
   "source": [
    "image_net_mean = [0.485, 0.456, 0.406]\n",
    "image_net_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "def image_transformation(img_dim, lanczos=True):\n",
    "    \"\"\"simple transformation/pre-processing of image data.\"\"\"\n",
    "\n",
    "    if lanczos:\n",
    "        resample_method = Image.LANCZOS\n",
    "    else:\n",
    "        resample_method = Image.BILINEAR\n",
    "\n",
    "    normalize = transforms.Normalize(mean=image_net_mean, std=image_net_std)\n",
    "    img_transforms = dict()\n",
    "    img_transforms['train'] = transforms.Compose([transforms.Resize((img_dim, img_dim), resample_method),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  normalize])\n",
    "\n",
    "    # Use same transformations as in train (since no data-augmentation is applied in train)\n",
    "    img_transforms['test'] = img_transforms['train']\n",
    "    img_transforms['val'] = img_transforms['train']\n",
    "    img_transforms['rest'] = img_transforms['train']\n",
    "    return img_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hicscIFVnFjZ"
   },
   "source": [
    "function that transforms features to target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88vyAP0c9qbt"
   },
   "outputs": [],
   "source": [
    "def transform_emotion(target_values, image_name, temp_img):\n",
    "    # read image and store in temporary file in which we store the changed temporary file after each manipulation step\n",
    "    # use a temporary file because some functions use cv2 and others PIL, also we do not want to overwrite the original file\n",
    "    # returns manipulated image\n",
    "    im = cv2.imread(image_name)\n",
    "    cv2.imwrite(temp_img, im)\n",
    "\n",
    "    # change feature values to mean values of target emotion, result is written to temp_img\n",
    "    change_bgr(temp_img, target_values[['blue', 'green', 'red']].values[0])\n",
    "    if np.all(target_values[['blue', 'green', 'red']].values[0]==0):\n",
    "        change_hsv(temp_img, target_values[['hue', 'saturation', 'brightness']].values[0], change_hue=True)\n",
    "    else:\n",
    "        change_hsv(temp_img, target_values[['hue', 'saturation', 'brightness']].values[0], change_hue=False)\n",
    "  \n",
    "    x = change_laplacian(temp_img, target_values['Laplacian'].values)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3lYBWDA-S9d"
   },
   "outputs": [],
   "source": [
    "def emotion_transform_e6(path, groundtruth, emotions, model, mapping):\n",
    "    \"\"\"\n",
    "    path = path to folder in which there is a folder for each emotion6 class except neutral\n",
    "    groundtruth = dataframe with the same structure as the emotion6 groundtruth dataframe, contains all images that should be used for the transformation\n",
    "    emotions = list of emotions \n",
    "    model = model for classification\n",
    "    mapping = df with entry for each pair of gt-emotion and target emotion that contains which features should be transformed to which value\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    folders = listdir(path)[:-1]\n",
    "    # for each folder (folders are named with emotions which are not necessarily the most probable emotion for the images in the folder):\n",
    "    for f in folders:\n",
    "        emotion_folder_path = p_join(path, f)\n",
    "        try:\n",
    "            l = listdir(emotion_folder_path)\n",
    "        except:\n",
    "            print(emotion_folder_path, \"is not a folder\")\n",
    "        print(emotion_folder_path)\n",
    "        # for each image:\n",
    "        for file in l:\n",
    "            image_id = file.split(\".\")[0]\n",
    "            image_name = p_join(emotion_folder_path, file)\n",
    "            im = cv2.imread(image_name)\n",
    "            #print(f\"before transformation:\\nhue={row['hue']:.2f}, brightness={row['brightness']:.2f}, saturation={row['saturation']:.2f}, red={row['red']:.2f}, blue={row['blue']:.2f}, green={row['green']:.2f}, contrast2={row['contrast2']:.2f}, laplacian={row['Laplacian']:.2f}\")\n",
    "\n",
    "            # retrieve ground truth distribution for current image\n",
    "            gt = groundtruth[(groundtruth['folder']==f) & (groundtruth['image']==int(image_id))]\n",
    "            if gt.empty:\n",
    "                continue\n",
    "            probabilities = gt.values[0,4:] # ground truth probabilities\n",
    "          \n",
    "            # compute most likely emotion\n",
    "            label = emotions[np.argmax(probabilities)]\n",
    "            # predict emotion before transformation with model\n",
    "            predicted_in, _ = classify(image_name, model, emotions)\n",
    "\n",
    "            # all possible target emotions\n",
    "            for target in emotions:\n",
    "                if target==label:\n",
    "                    continue\n",
    "                target_values = mapping[(mapping['original']==label) & (mapping['target']==target)]\n",
    "                temp_img = p_join(path, \"temp.jpg\")\n",
    "                #print(f\"goal {label} to {target}:\\nhue={target_values['hue'].values[0]:.2f}, brightness={target_values['brightness'].values[0]:.2f}, saturation={target_values['saturation'].values[0]:.2f}, red={target_values['red'].values[0]:.2f}, blue={target_values['blue'].values[0]:.2f}, green={target_values['green'].values[0]:.2f}, contrast2={target_values['contrast2'].values[0]:.2f}, laplacian={target_values['Laplacian'].values[0]:.2f}\")\n",
    "                transform_emotion(target_values, image_name, temp_img)\n",
    "\n",
    "                row = {}\n",
    "                row['folder'] = f\n",
    "                row['image'] = file #image name\n",
    "                row['Emotion'] = label #most likely emotion\n",
    "                row['target'] = target #target emotion\n",
    "              \n",
    "                # predict emotion after transformation\n",
    "                predicted_out, probabilities_out = classify(temp_img, model, emotions)\n",
    "                #print(\"before:\", label, \"wanted:\", target, \"after:\", predicted_out)\n",
    "                row['predicted_in'] = predicted_in\n",
    "                row['predicted_out'] = predicted_out\n",
    "                # add image results to dataframe\n",
    "                df = df.append(row, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrnldZGknDqj"
   },
   "source": [
    "example for one image:\n",
    "transformation + classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgECJyn8Q7YE"
   },
   "outputs": [],
   "source": [
    "em = \"anger\"\n",
    "dir = p_join(EM_all, em)\n",
    "groundtruth = emotion_distr\n",
    "emotions = emotions_e6+['neutral']\n",
    "for target in emotions:\n",
    "    count = 0\n",
    "    for img in listdir(dir)[8:]:\n",
    "        if count == 1:\n",
    "            break\n",
    "        count += 1\n",
    "\n",
    "        img_name = p_join(dir, img)\n",
    "        image_id = img.split(\".\")[0]\n",
    "        # retrieve ground truth distribution for current image\n",
    "        gt = groundtruth[(groundtruth['folder']==em) & (groundtruth['image']==int(image_id))]\n",
    "        print(image_id, dir)\n",
    "        probabilities = gt.values[0,4:] # ground truth probabilities\n",
    "        # compute most likely emotion\n",
    "        label = emotions[np.argmax(probabilities)]\n",
    "        if target==label: continue\n",
    "\n",
    "        im = cv2.imread(img_name)\n",
    "        row = {}\n",
    "        row = calculateFeatures(row, im)\n",
    "        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(im)\n",
    "        plt.show()\n",
    "        print(label, 'to', target)\n",
    "        print(f\"before transformation:\\nhue={row['hue']:.2f}, brightness={row['brightness']:.2f}, saturation={row['saturation']:.2f}, \\nred={row['red']:.2f}, blue={row['blue']:.2f}, green={row['green']:.2f},\\nlaplacian={row['Laplacian']:.2f}\")\n",
    "\n",
    "        target_values = mapping[(mapping['original']==label) & (mapping['target']==target)]\n",
    "        print(f\"goal:\\nhue={target_values['hue'].values[0]:.2f}, brightness={target_values['brightness'].values[0]:.2f}, saturation={target_values['saturation'].values[0]:.2f}, \\nred={target_values['red'].values[0]:.2f}, blue={target_values['blue'].values[0]:.2f}, green={target_values['green'].values[0]:.2f},\\nlaplacian={target_values['Laplacian'].values[0]:.2f}\")\n",
    "\n",
    "        temp_img = p_join(EM_all, \"test_\"+target+\".jpg\")\n",
    "        transform_emotion(target_values, img_name, temp_img) #transform_emotion(target_values, emotion_folder_path, file, temp_path)\n",
    "        print(\"assigned=\", classify(temp_img, model, emotions)[0])\n",
    "        row = {}\n",
    "        im = cv2.imread(temp_img)\n",
    "        row = calculateFeatures(row, im)\n",
    "        print(f\"after transformation:\\nhue={row['hue']:.2f}, brightness={row['brightness']:.2f}, saturation={row['saturation']:.2f}, \\nred={row['red']:.2f}, blue={row['blue']:.2f}, green={row['green']:.2f},\\nlaplacian={row['Laplacian']:.2f}\")\n",
    "        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(im)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "io4dKjJanMFc"
   },
   "source": [
    "### Evaluating the transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrFYK55Sz-w2"
   },
   "outputs": [],
   "source": [
    "def compute_all(path_to_data, emotions, model, feature_df, test_df, use_pvals=2):\n",
    "    \"\"\"\n",
    "    function that takes images in ground truth data frame test_df from \"path_to_data\", transforms them according to the features in features_df by\n",
    "    calculating p-values and mean feature values per emotion.\n",
    "    path_to_data = path to folder in which there is a folder for each emotion\n",
    "    emotions = list of emotions in the dataset\n",
    "    model = model for classification of emotion\n",
    "    feature_df = data frame that contains the features extracted from the training split\n",
    "    test_df = the lines in the ground truth data frame that contain the images we want to transform\n",
    "    use_pvals = True/False whether to use only the features for which the p-value that reflect the correlation between emotion prob. and feature value is smaller than 0.05\n",
    "    \"\"\"\n",
    "    # calculate p-values to get relevant features\n",
    "    p_val = feature_df.corr(spearmanr_pval)[feature_df.corr().columns[3:10]].T[feature_df.corr().columns[10:]]\n",
    "\n",
    "    # get for each emotion the p-values of the features we want to transform\n",
    "    p_val_short = get_p_val(feature_df)\n",
    "\n",
    "    # get the mean feature values for each emotion\n",
    "    df_em6_means = get_means_em6(feature_df, emotions_e6)\n",
    "\n",
    "    # combine p-vals and means to get only the means of the relevant features \n",
    "    mapping = create_mapping_dictionary(p_val_short, df_em6_means, emotions, use_pvals)\n",
    "\n",
    "    print(\"transform images\")\n",
    "    # transform and classify test images\n",
    "    df=emotion_transform_e6(path_to_data, test_df, emotions, model, mapping)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCkpEYzdRfd0"
   },
   "source": [
    "## Evaluation\n",
    "#### first dataset\n",
    "images with highest prob > 0.5 for training (feature extraction), all others for testing (feature transformation)\n",
    "\n",
    "1. only with p-val < 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Isc7SSSIxzT9"
   },
   "outputs": [],
   "source": [
    "emotions = emotions_e6+['neutral']\n",
    "\n",
    "# load already calculated features for training images\n",
    "df_major = pd.read_csv(p_join(PROJDIR, \"emotion6_features_majoremotion.csv\"), sep=',')\n",
    "\n",
    "# get subset of ground truth dataframe for all images with hightest probability smaller than 0.5\n",
    "minor_prob_em6 = threshold_images(emotion_distr, threshold = 0.5, greater_than = False)\n",
    "print(len(df_major), \"images for feature extraction (train)\")\n",
    "print(len(minor_prob_em6), \"images for transformation (test)\")\n",
    "\n",
    "df_strongtrain_weaktest = compute_all(EM_all, emotions_e6+['neutral'], model, df_major, minor_prob_em6, 2)\n",
    "df_strongtrain_weaktest.to_csv(p_join(PROJDIR, 'df_strongtrain_weaktest_pvaluefilter.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJXDbZytMHqa"
   },
   "outputs": [],
   "source": [
    "df_strongtrain_weaktest = pd.read_csv(p_join(PROJDIR, 'df_strongtrain_weaktest_pvaluefilter.csv'), sep=\",\")\n",
    "print(len(df_major), \"images for feature extraction (train)\")\n",
    "print(len(minor_prob_em6), \"images for transformation (test)\")\n",
    "print(\"correctly classified before transformation \\t\", (df_strongtrain_weaktest['Emotion'].values == df_strongtrain_weaktest[\"predicted_in\"].values).sum() / len(df_strongtrain_weaktest))\n",
    "print(\"classified as target before transformation \\t\", (df_strongtrain_weaktest[\"target\"].values == df_strongtrain_weaktest[\"predicted_in\"].values).sum() / len(df_strongtrain_weaktest))\n",
    "print(\"classified as target after transformation \\t\", (df_strongtrain_weaktest[\"target\"].values == df_strongtrain_weaktest[\"predicted_out\"].values).sum() / len(df_strongtrain_weaktest))\n",
    "print(\"fraction were prediction did not change \\t\", (df_strongtrain_weaktest[\"predicted_in\"].values == df_strongtrain_weaktest[\"predicted_out\"].values).sum() / len(df_strongtrain_weaktest))\n",
    "\n",
    "for e in emotions:\n",
    "    print(\"classified as\", e, \"before transformation:\\t\", ((df_strongtrain_weaktest[\"target\"] == e) & (df_strongtrain_weaktest[\"predicted_in\"] == e)).sum(),\n",
    "        \"\\tAfter transformation \", ((df_strongtrain_weaktest[\"target\"] == e) & (df_strongtrain_weaktest[\"predicted_out\"] == e)).sum(),\n",
    "        \"\\tNewly after transformation \", ((df_strongtrain_weaktest[\"target\"] == e) & (df_strongtrain_weaktest[\"predicted_out\"] == e) & (df_strongtrain_weaktest[\"predicted_in\"] != e)).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RI-RB8c6TfaZ"
   },
   "source": [
    "2. all features independant of p-val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJDfh_ds4o9w"
   },
   "outputs": [],
   "source": [
    "df_strongtrain_weaktest = compute_all(EM_all, emotions_e6+['neutral'], model, df_major, minor_prob_em6, 0)\n",
    "df_strongtrain_weaktest.to_csv(p_join(PROJDIR, 'df_strongtrain_weaktest_nopvaluefilter.csv'))\n",
    "\n",
    "df_strongtrain_weaktest = pd.read_csv(p_join(PROJDIR, 'df_strongtrain_weaktest_nopvaluefilter.csv'))\n",
    "print(len(df_major), \"images for feature extraction (train)\")\n",
    "print(len(minor_prob_em6), \"images for transformation (test)\")\n",
    "print(\"correctly classified before transformation \\t\", (df_strongtrain_weaktest['Emotion'].values == df_strongtrain_weaktest[\"predicted_in\"].values).sum() / len(df_strongtrain_weaktest))\n",
    "print(\"classified as target before transformation \\t\", (df_strongtrain_weaktest[\"target\"].values == df_strongtrain_weaktest[\"predicted_in\"].values).sum() / len(df_strongtrain_weaktest))\n",
    "print(\"classified as target after transformation \\t\", (df_strongtrain_weaktest[\"target\"].values == df_strongtrain_weaktest[\"predicted_out\"].values).sum() / len(df_strongtrain_weaktest))\n",
    "print(\"fraction were prediction did not change \\t\", (df_strongtrain_weaktest[\"predicted_in\"].values == df_strongtrain_weaktest[\"predicted_out\"].values).sum() / len(df_strongtrain_weaktest))\n",
    "\n",
    "for e in emotions:\n",
    "    print(\"classified as\", e, \"before transformation:\\t\", ((df_strongtrain_weaktest[\"target\"] == e) & (df_strongtrain_weaktest[\"predicted_in\"] == e)).sum(),\n",
    "        \"\\tAfter transformation \", ((df_strongtrain_weaktest[\"target\"] == e) & (df_strongtrain_weaktest[\"predicted_out\"] == e)).sum(),\n",
    "        \"\\tNewly after transformation \", ((df_strongtrain_weaktest[\"target\"] == e) & (df_strongtrain_weaktest[\"predicted_out\"] == e) & (df_strongtrain_weaktest[\"predicted_in\"] != e)).sum())\n",
    "from sklearn import metrics\n",
    "confusion_matrix = metrics.confusion_matrix(df_strongtrain_weaktest[\"target\"], df_strongtrain_weaktest[\"predicted_out\"])\n",
    "plt.figure(figsize=(8,6))\n",
    "emsorted = emotions\n",
    "emsorted.sort()\n",
    "out_pdf = p_join(PROJDIR,'confusionmatrix_em6.pdf')\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(out_pdf)\n",
    "sns.heatmap(confusion_matrix, annot=True, xticklabels=emsorted, yticklabels=emsorted)\n",
    "plt.ylabel(\"target\")\n",
    "plt.xlabel(\"predicted\")\n",
    "pdf.savefig()\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdQvdOGyfK_t"
   },
   "source": [
    "3. only p-val for target emotion are relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4L4vLL1AfKVz"
   },
   "outputs": [],
   "source": [
    "df_strongtrain_weaktest = compute_all(EM_all, emotions_e6+['neutral'], model, df_major, minor_prob_em6, 1)\n",
    "df_strongtrain_weaktest.to_csv(p_join(PROJDIR, 'df_strongtrain_weaktest_targetpvaluefilter.csv'))\n",
    "\n",
    "df_strongtrain_weaktest = pd.read_csv(p_join(PROJDIR, 'df_strongtrain_weaktest_targetpvaluefilter.csv'), sep=\",\")\n",
    "print(len(df_major), \"images for feature extraction (train)\")\n",
    "print(len(minor_prob_em6), \"images for transformation (test)\")\n",
    "print(\"correctly classified before transformation \\t\", (df_strongtrain_weaktest['Emotion'].values == df_strongtrain_weaktest[\"predicted_in\"].values).sum() / len(df_strongtrain_weaktest))\n",
    "print(\"classified as target before transformation \\t\", (df_strongtrain_weaktest[\"target\"].values == df_strongtrain_weaktest[\"predicted_in\"].values).sum() / len(df_strongtrain_weaktest))\n",
    "print(\"classified as target after transformation \\t\", (df_strongtrain_weaktest[\"target\"].values == df_strongtrain_weaktest[\"predicted_out\"].values).sum() / len(df_strongtrain_weaktest))\n",
    "print(\"fraction were prediction did not change \\t\", (df_strongtrain_weaktest[\"predicted_in\"].values == df_strongtrain_weaktest[\"predicted_out\"].values).sum() / len(df_strongtrain_weaktest))\n",
    "\n",
    "for e in emotions:\n",
    "    print(\"classified as\", e, \"before transformation:\\t\", ((df_strongtrain_weaktest[\"target\"] == e) & (df_strongtrain_weaktest[\"predicted_in\"] == e)).sum(),\n",
    "        \"\\tAfter transformation \", ((df_strongtrain_weaktest[\"target\"] == e) & (df_strongtrain_weaktest[\"predicted_out\"] == e)).sum(),\n",
    "        \"\\tNewly after transformation \", ((df_strongtrain_weaktest[\"target\"] == e) & (df_strongtrain_weaktest[\"predicted_out\"] == e) & (df_strongtrain_weaktest[\"predicted_in\"] != e)).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1GpNq9zwJiY"
   },
   "source": [
    "### using the train/val split from Anmol's classification model as train/test\n",
    "\n",
    "1. using only features with p-value < 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iev-EnBOwIjF"
   },
   "outputs": [],
   "source": [
    "trainsplit_df = pd.read_csv(p_join(PROJDIR, \"emotion6_features_trainingsplitanmolsmodel.csv\"))\n",
    "test_em6 = dataframe_from_model(emotion_distr, emotion_distr_anmol, split_class=\"val\")\n",
    "df_traintrain_valtest = compute_all(EM_all, emotions_e6+['neutral'], model, trainsplit_df, test_em6, 2)\n",
    "df_traintrain_valtest.to_csv(p_join(PROJDIR, 'df_traintrain_valtest_pvaluefilter.csv'))\n",
    "\n",
    "df_traintrain_valtest = pd.read_csv(p_join(PROJDIR, 'df_traintrain_valtest_pvaluefilter.csv'), sep=\",\")\n",
    "print(len(trainsplit_df), \"images for feature extraction (train)\")\n",
    "print(len(test_em6), \"images for transformation (test)\")\n",
    "print(\"correctly classified before transformation \\t\", (df_traintrain_valtest['Emotion'].values == df_traintrain_valtest[\"predicted_in\"].values).sum() / len(df_traintrain_valtest))\n",
    "print(\"classified as target before transformation \\t\", (df_traintrain_valtest[\"target\"].values == df_traintrain_valtest[\"predicted_in\"].values).sum() / len(df_traintrain_valtest))\n",
    "print(\"classified as target after transformation \\t\", (df_traintrain_valtest[\"target\"].values == df_traintrain_valtest[\"predicted_out\"].values).sum() / len(df_traintrain_valtest))\n",
    "print(\"fraction were prediction did not change \\t\", (df_traintrain_valtest[\"predicted_in\"].values == df_traintrain_valtest[\"predicted_out\"].values).sum() / len(df_traintrain_valtest))\n",
    "for e in emotions:\n",
    "    print(\"classified as target\", e, \"before transformation:\\t\", ((df_traintrain_valtest[\"target\"] == e) & (df_traintrain_valtest[\"predicted_in\"] == e)).sum(),\"\\tAfter transformation \", ((df_traintrain_valtest[\"target\"] == e) & (df_traintrain_valtest[\"predicted_out\"] == e)).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4J-tlyxHkAo"
   },
   "source": [
    "2. with all features (no p-value filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5rXmJ1FHjgv"
   },
   "outputs": [],
   "source": [
    "df_traintrain_valtest = compute_all(EM_all, emotions_e6+['neutral'], model, trainsplit_df, test_em6, 0)\n",
    "df_traintrain_valtest.to_csv(p_join(PROJDIR, 'df_traintrain_valtest_nopvaluefilter.csv'))\n",
    "\n",
    "df_traintrain_valtest = pd.read_csv(p_join(PROJDIR, 'df_traintrain_valtest_nopvaluefilter.csv'), sep=\",\")\n",
    "print(len(trainsplit_df), \"images for feature extraction (train)\")\n",
    "print(len(test_em6), \"images for transformation (test)\")\n",
    "print(\"correctly classified before transformation \\t\", (df_traintrain_valtest['Emotion'].values == df_traintrain_valtest[\"predicted_in\"].values).sum() / len(df_traintrain_valtest))\n",
    "print(\"classified as target before transformation \\t\", (df_traintrain_valtest[\"target\"].values == df_traintrain_valtest[\"predicted_in\"].values).sum() / len(df_traintrain_valtest))\n",
    "print(\"classified as target after transformation \\t\", (df_traintrain_valtest[\"target\"].values == df_traintrain_valtest[\"predicted_out\"].values).sum() / len(df_traintrain_valtest))\n",
    "print(\"fraction were prediction did not change \\t\", (df_traintrain_valtest[\"predicted_in\"].values == df_traintrain_valtest[\"predicted_out\"].values).sum() / len(df_traintrain_valtest))\n",
    "for e in emotions:\n",
    "    print(\"classified as target\", e, \"before transformation:\\t\", ((df_traintrain_valtest[\"target\"] == e) & (df_traintrain_valtest[\"predicted_in\"] == e)).sum(),\"\\tAfter transformation \", ((df_traintrain_valtest[\"target\"] == e) & (df_traintrain_valtest[\"predicted_out\"] == e)).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sv2CqTDzk2Ko"
   },
   "source": [
    "3. only target emotion p-values are relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVin2edPkW6o"
   },
   "outputs": [],
   "source": [
    "df_traintrain_valtest = compute_all(EM_all, emotions_e6+['neutral'], model, trainsplit_df, test_em6, 1)\n",
    "df_traintrain_valtest.to_csv(p_join(PROJDIR, 'df_traintrain_valtest_targetpvaluefilter.csv'))\n",
    "\n",
    "df_traintrain_valtest = pd.read_csv(p_join(PROJDIR, 'df_traintrain_valtest_targetpvaluefilter.csv'), sep=\",\")\n",
    "print(\"correctly classified before transformation \\t\", (df_traintrain_valtest['Emotion'].values == df_traintrain_valtest[\"predicted_in\"].values).sum() / len(df_traintrain_valtest))\n",
    "print(\"classified as target before transformation \\t\", (df_traintrain_valtest[\"target\"].values == df_traintrain_valtest[\"predicted_in\"].values).sum() / len(df_traintrain_valtest))\n",
    "print(\"classified as target after transformation \\t\", (df_traintrain_valtest[\"target\"].values == df_traintrain_valtest[\"predicted_out\"].values).sum() / len(df_traintrain_valtest))\n",
    "print(\"fraction were prediction did not change \\t\", (df_traintrain_valtest[\"predicted_in\"].values == df_traintrain_valtest[\"predicted_out\"].values).sum() / len(df_traintrain_valtest))\n",
    "for e in emotions:\n",
    "  print(\"classified as target\", e, \"before transformation:\\t\", ((df_traintrain_valtest[\"target\"] == e) & (df_traintrain_valtest[\"predicted_in\"] == e)).sum(),\"\\tAfter transformation \", ((df_traintrain_valtest[\"target\"] == e) & (df_traintrain_valtest[\"predicted_out\"] == e)).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jchMhoG5epus"
   },
   "source": [
    "---\n",
    "## APPENDIX\n",
    "testing the contrast transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFweMlzXervk"
   },
   "outputs": [],
   "source": [
    "from PIL import Image,ImageEnhance\n",
    "dir = p_join(EM_all, \"joy\")\n",
    "factors = [0.25,0.5, 1.5,  2]\n",
    "fig, ax = plt.subplots(1,len(factors), figsize=(14,5))\n",
    "\n",
    "for i, factor in enumerate(factors):\n",
    "  contrast_rms_before = []\n",
    "  contrast_glcm_before = []\n",
    "  contrast_rms_after = []\n",
    "  contrast_glcm_after = []\n",
    "  for img_name in listdir(dir):\n",
    "    \n",
    "    img_path = p_join(dir, img_name)\n",
    "    img=cv2.imread(img_path)\n",
    "    \n",
    "    #calculate both contrast measures\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    glcm = greycomatrix(gray, distances=[1], angles=[np.pi/2])\n",
    "    \n",
    "    contrast_glcm_before += [greycoprops(glcm, \"contrast\")[0,0]]\n",
    "    contrast_rms_before += [gray.std()]\n",
    "    \n",
    "    img = Image.open(img_path)\n",
    "    if img.mode is not 'RGB':\n",
    "      img = img.convert('RGB')\n",
    "\n",
    "    img_contr_obj=ImageEnhance.Contrast(img)\n",
    "    e_img=img_contr_obj.enhance(factor)\n",
    "    e_img.save(p_join(EM_all, \"temp.png\"))\n",
    "\n",
    "    img_path=p_join(EM_all, \"temp.png\")\n",
    "    img=cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    glcm = greycomatrix(gray, distances=[1], angles=[np.pi/2])\n",
    "    contrast_glcm_after += [greycoprops(glcm, \"contrast\")[0,0]]\n",
    "    contrast_rms_after += [gray.std()]\n",
    "\n",
    "  factors_glcm = np.array(contrast_glcm_after) / np.array(contrast_glcm_before)\n",
    "  factors_rms = np.array(contrast_rms_after) / np.array(contrast_rms_before)\n",
    "  ax[i].boxplot([factors_glcm, factors_rms] , labels=[\"glcm\", \"rms\"])\n",
    "  ax[i].set_title(f\"change in contrast with factor {factor}\")\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZarvIsPmDkIF"
   },
   "source": [
    "---\n",
    "transform images for the survey. use the best model (features from training set of classification model)\n",
    "survey images are chosen to be not used for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIwlWprQMEa9"
   },
   "outputs": [],
   "source": [
    "EM_survey = p_join(DATA, \"surveyimages\")\n",
    "df_major = pd.read_csv(p_join(PROJDIR, \"emotion6_features_majoremotion.csv\"), sep=',')\n",
    "p_val = get_p_val(df_major)\n",
    "means = get_means_em6(df_major, emotions_e6)\n",
    "mapping = create_mapping_dictionary(p_val, means, emotions, use_pval=True)\n",
    "\n",
    "for file in listdir(EM_survey):\n",
    "    file_name = file.split(\".\")[0]\n",
    "    initial_emotion = file_name.split(\"_\")[1]\n",
    "    target = \"sadness\"\n",
    "    img_path = p_join(EM_survey, file)\n",
    "    print(img_path)\n",
    "    if initial_emotion in [\"anger\", \"fear\", \"sadness\", \"disgust\"]:\n",
    "        target = \"joy\"\n",
    "    elif initial_emotion not in [\"joy\", \"surprise\"]:\n",
    "        print(\"not a valid image\")\n",
    "        continue\n",
    "\n",
    "    target_values = mapping[(mapping['original']==initial_emotion) & (mapping['target']==target)]\n",
    "    target_img = p_join(p_join(EM_survey, \"model_theresa\"), file_name+\"_\"+target+\".jpg\")\n",
    "    print(target_img, \"\\n\")\n",
    "    transform_emotion(target_values, img_path, target_img)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "emotion6_probabilities.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
